2542662402_d781dd7f7c
3316725440_9ccd9b5417
Test  image one - 3385593926_d3e9c21170.jpg
im = '/content/drive/MyDrive/MasterCode/Flickr8k_Dataset/3320032226_63390d74a6.jpg'


# evaluate the skill of the model
def evaluate_model_basic_trained_images(encoder_final_quant,encoder_interpreter,decoder_interpreter):
  actual, predicted = list(), list()
  cnt = 0
  start = time.time()
  for key, desc_list in test_d.items():
    cnt= cnt+1
    print(cnt)
    if encoder_final_quant == 1:
      print("\n ****** NOT VALID Encoder Only Quantization: ") 
      #yhat = predict_captions_interpreter_encoder(key,encoder_interpreter)
    elif encoder_final_quant == 2:
      print("\n ****** Encoder Quantization and Final Model Quantization: ")
      yhat = predict_captions_interpreter_encoder_full(key,encoder_interpreter,decoder_interpreter,False)
    elif encoder_final_quant == 3:
      print("\n ****** NOT VALID Encoder and Final Model Quantization: ")
      #yhat = predict_captions_interpreter_encoder_full(key,encoder_interpreter,decoder_interpreter,True)
    else:
      print("\n ****** Model Quantization: ")
      yhat = predict_captions(key)

    references = [desp.split() for desp in desc_list]
    actual.append(references)
    predicted.append(yhat.split())
  
  #print('FULL ACTUAL- TEXT: ')
  #print('FULL PREDICTED- TEXT: ')
  print("\n****** Time taken: ", (time.time()-start)/60)  
	# calculate BLEU score
  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))
  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))
  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))
  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))


test_d = {}
for i in test_img:
    if i[len(images):] in d:
        test_d[i] = d[i[len(images):]]
        
        
actual, predicted = list(), list()
    start = time.time()
    if encoder_final_quant == 1:
        print("\n ****** Encoder Only Quantization: ") 
        yhat = predict_captions_interpreter_encoder(img_path,encoder_interpreter)
    elif encoder_final_quant == 2:
        print("\n ****** No Encoder Quantization and Final Model Quantization: ")
        yhat = predict_captions_interpreter_encoder_full(img_path,encoder_interpreter,decoder_interpreter,False)
    elif encoder_final_quant == 3:
        print("\n ****** Both Encoder and Final Model Quantization: ")
        yhat = predict_captions_interpreter_encoder_full(img_path,encoder_interpreter,decoder_interpreter,True)
    else:
        print("\n ******NO Model Quantization: ")
        yhat = predict_captions(img_path)

    print("\n****** Time taken: ", (time.time()-start)/60)  

    references = [desp.split() for desp in d[img_str_key]]
    actual.append(references)
    predicted.append(yhat.split())
    print('ACTUAL- TEXT:')
    print(d[img_str_key])
    #predicted_text = ' '.join(predicted[1:-1]
    print('PREDICTED- TEXT:')
    print(yhat)


def beam_search_predictions_encoder_interpreter(image,interpreter, beam_index = 3):
    start = [word2idx["<start>"]]
    
    start_word = [[start, 0.0]]
    
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            par_caps = sequence.pad_sequences([s[0]], maxlen=max_len, padding='post')
            output = extract_features_quantized(interpreter,image)      
            preds = final_model.predict([output, np.array(par_caps)])
            #e = encoding_test[image[len(images):]]
            #preds = final_model.predict([np.array([e]), np.array(par_caps)])
            word_preds = np.argsort(preds[0])[-beam_index:]
            
            # Getting the top <beam_index>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                prob += preds[0][w]
                temp.append([next_cap, prob])
                    
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])
        # Getting the top words
        start_word = start_word[-beam_index:]
    
    start_word = start_word[-1][0]
    intermediate_caption = [idx2word[i] for i in start_word]

    final_caption = []
    
    for i in intermediate_caption:
        if i != '<end>':
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption

def beam_search_predictions_encoder_interpreter(image,interpreter, beam_index = 3):
    start = [word2idx["<start>"]]
    
    start_word = [[start, 0.0]]
    
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            par_caps = sequence.pad_sequences([s[0]], maxlen=max_len, padding='post')
            output = extract_features_quantized(interpreter,image)      
            preds = final_model.predict([output, np.array(par_caps)])
            #e = encoding_test[image[len(images):]]
            #preds = final_model.predict([np.array([e]), np.array(par_caps)])
            word_preds = np.argsort(preds[0])[-beam_index:]
            
            # Getting the top <beam_index>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                prob += preds[0][w]
                temp.append([next_cap, prob])
                    
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])
        # Getting the top words
        start_word = start_word[-beam_index:]
    
    start_word = start_word[-1][0]
    intermediate_caption = [idx2word[i] for i in start_word]

    final_caption = []
    
    for i in intermediate_caption:
        if i != '<end>':
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption

#im = '/content/drive/MyDrive/MasterCode/Flickr8k_Dataset/3320032226_63390d74a6.jpg'
im = '/content/drive/MyDrive/MasterCode/Flickr8k_Dataset/2542662402_d781dd7f7c.jpg'
print ('Normal Max search:', predict_captions_interpreter_encoder(im,interpreter))
Image.open(im)
#print('Beam Search, k=3:', beam_search_predictions_encoder_interpreter(im,interpreter, beam_index=3))
#print('Beam Search, k=5:', beam_search_predictions_encoder_interpreter(im,interpreter, beam_index=5))
#print('Beam Search, k=7:', beam_search_predictions_encoder_interpreter(im,interpreter, beam_index=7))



def beam_search_predictions(image, beam_index = 3):
    start = [word2idx["<start>"]]
    
    start_word = [[start, 0.0]]
    
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            par_caps = sequence.pad_sequences([s[0]], maxlen=max_len, padding='post')
            e = encoding_test[image[len(images):]]
            preds = final_model.predict([np.array([e]), np.array(par_caps)])
            
            word_preds = np.argsort(preds[0])[-beam_index:]
            
            # Getting the top <beam_index>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                prob += preds[0][w]
                temp.append([next_cap, prob])
                    
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])
        # Getting the top words
        start_word = start_word[-beam_index:]
    
    start_word = start_word[-1][0]
    intermediate_caption = [idx2word[i] for i in start_word]

    final_caption = []
    
    for i in intermediate_caption:
        if i != '<end>':
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption




def extract_features_quantized(extractor,file,ts=(299, 299)):


def extract_final_model_quantized(final_extractor,feature_output,caps_out):
